---
title: "In-Class Exercise 4"
author: "KB"
editor: visual
execute: 
  warning: false
---

First published on: 10-Dec-2022

# 6 Calibrate Hedonic Pricing Model for Private Highrise Property with GWR Method

**(with additional notes)**

## 6.1 Overview

Geographically Weighted Regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we will learn how to build [hedonic pricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.

## 6.2 The Data

Two data sets are used in this model building exercise, they are:

-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*)

-   condo_resale_2015 in csv format (i.e. *condo_resale_2015.csv*)

## 6.3 Getting Started

Before we get started, it is important for us to install the necessary R packages and launch these R packages into R environment.

The R packages needed for this exercise are as follows:

-   R package for building OLS and performing diagnostics tests - [**olsrr**](https://olsrr.rsquaredacademy.com/)

-   R package for calibrating geographical weighted family of models - [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/)

-   R package for multivariate data visualisation and analysis - [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)

-   Spatial data handling - **sf**

-   Attribute data handling - **tidyverse**, especially **readr**, **ggplot2** and **dplyr**

-   Choropleth mapping - **tmap**

The code chunk below installs and launches these R packages into R environment.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

## 6.4 A brief note on GWmodel

[**GWmodel**](https://www.jstatsoft.org/article/view/v063i17) package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW Principal Components Analysis, GW Discriminant Analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful data exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.

## 6.5 Geospatial Data Wrangling

### 6.5.1 Import geospatial data

The geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014's planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in SVY21 projected coordinates systems.

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using `st_read()` of **sf** packages.

```{r}
mpsz = st_read(dsn = "In-Class_Ex4/data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

The report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called *mpsz* and it is a simple feature object. The geometry type is multipolygon. it is also important to note that *mpsz* sf object does not have EPSG information.

### 6.5.2 Update CRS information

The code chunk below updates the newly imported *mpsz* with the appropriate ESPG code (i.e. 3414), which is slightly different from the SVY21 coordinate system.

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

::: callout-note
## We could have transformed the coordinates while reading in the *MP14_SUBZONE_WEB_PL* file into R in the previous section by adding the *CRS* argument.
:::

After transforming the projection metadata, we can verify the projection of the newly transformed *mpsz_svy21* by using `st_crs()` of **sf** package.

The code chunk below will be used to varify the newly transformed *mpsz_svy21*.

```{r}
st_crs(mpsz_svy21)
```

Notice that the EPSG: is indicated as *3414* now.

Next, we reveal the extent of *mpsz_svy21* by using `st_bbox()` of **sf** package. The st_bblox() bounds the sf object in a box

```{r}
st_bbox(mpsz_svy21) #view extent
```

## 6.6 Aspatial Data Wrangling

### 6.6.1 Import the aspatial data

The *condo_resale_2015* is in csv file format. The codes chunk below uses `read_csv()` function of **readr** package to import *condo_resale_2015* into R as a tibble data frame called *condo_resale*.

```{r}
condo_resale = read_csv("In-Class_Ex4/data/aspatial/Condo_resale_2015.csv", show_col_types = FALSE)
```

After importing the data file into R, it is important for us to examine if the data file has been imported correctly.

The codes chunks below uses `glimpse()` to display the data structure of *condo_resale* data frame

```{r}
glimpse(condo_resale)
```

```{r}
# see the data in XCOORD column
head(condo_resale$LONGITUDE)
```

```{r}
#see the data in YCOORD column
head(condo_resale$LATITUDE) 
```

We use `summary()` of base R is used to display the summary statistics of *cond_resale* tibble data frame.

```{r}
summary(condo_resale)
```

### 6.6.2 Converting aspatial data frame into a sf object

Currently, the *condo_resale* tibble data frame is aspatial. We will convert it to a **sf** object. The code chunk below converts *condo_resale* data frame into a simple feature data frame by using `st_as_sf()` of **sf** packages.

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

Notice that `st_transform()` of **sf** package is used to convert the coordinates from wgs84 (i.e. crs:4326) to SVY1 (i.e. crs=3414).

Next, `head()` is used to list the content of *condo_resale.sf* object.

```{r}
head(condo_resale.sf)
```

Notice that the output (Geometry Type) is a point feature data frame.

## 6.7 Exploratory Data Analysis (EDA)

In the section, we learn how to use statistical graphics functions of **ggplot2** package to perform EDA.

### 6.7.1 EDA using statistical graphics

We plot the distribution of *SELLING_PRICE* as shown in the code chunk below.

```{r}
options(scipen=999)

ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue") + 
   ggtitle("Distribution of SELLING_RPICE")
```

The chart above reveals a right-skewed distribution. This means that more condominium units were transacted at relatively lower prices.

Statistically, the right-skewed distribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called *LOG_SELLING_PRICE* by using a log transformation on the variable *SELLING_PRICE*. It is performed using `mutate()` of **dplyr** package.

```{r}
# We add a constant +1 to avoid a situation where selling price = 0 and taking log(0) will result in -Inf
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE+1))
```

Now, we plot the *LOG_SELLING_PRICE* using the code chunk below.

```{r}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue") + 
   ggtitle("Distribution of LOG_SELLING_RPICE")
```

### 6.7.2 Multiple Histogram Plots distribution of variables

In this section,we learn how to draw a small multiple histograms (also known as Trellis plot) by using `ggarrange()` of [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/) package.

The code chunk below is used to create 12 histograms. Then, `ggarrange()` is used to organised these histogram into a 3 columns by 4 rows small multiple plot.

```{r fig.height= 8}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

### 6.7.3 Drawing Statistical Point Map

Lastly, we want to review the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using **tmap** package.

First, we will turn on the interactive mode of tmap by using the code chunk below.

```{r}
tmap_mode("view")
```

Next, the code chunks below is used to create an interactive point symbol map.

```{r}
#| warning: false
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE)+
  tm_polygons() +
  tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

::: callout-tip
If we see the "Error: Shape contains Invalid polygons. Please fix it or set tmap_options(check.and fix=True) and rerun the plot" message, we can insert suggested code line in the script (before the tm_polygon line is run).

In this case, the issue is due to 2 broken polygons in the base map (i.e. data issue).
:::

Notice that [`tm_dots()`](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_symbols) is used instead of `tm_bubbles()`.

`set.zoom.limits` argument of `tm_view()` sets the minimum and maximum zoom level to 11 and 14 respectively.

Before moving on to the next section, the code below will be used to turn R display into `plot` mode.

```{r}
tmap_mode("plot")
```

## 6.8 Hedonic Pricing Modelling in R

In this section, we learn how to building hedonic pricing models for condominium resale units using [`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/lm) of R base.

### 6.8.1 Simple Linear Regression Method

First, we will build a simple linear regression model by using *SELLING_PRICE* as the dependent variable and *AREA_SQM* as the independent variable.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

`lm()` returns an object of class "lm" or for multiple responses of class c("mlm", "lm").

The functions `summary()` and `anova()` can be used to obtain and print a summary and analysis of variance table of the results.

```{r}
summary(condo.slr)
```

```{r}
anova(condo.slr)
```

The output report reveals that the SELLING_PRICE can be explained by using the formula:

          *y = -258121.1 + 14719x1* where x1 is the AREA_SQM

The R-squared (*aka* Coefficient of Determination) of 0.4518 reveals that the simple regression model built is able to explain about 45% of the variation in selling prices.

Y = 𝛼 + 𝛽X

[ANOVA of mean:]{.underline}

The Analysis of Variance report provides the calculations for comparing the fitted model to a simple mean model. The hypotheses for the F-test are:

H0: 𝛽1 = 𝛽2 =. . . . = 𝛽𝑘 = 0

H1: N𝑜𝑡 𝑎𝑙𝑙 𝑒𝑞𝑢𝑎𝑙 𝑡𝑜 0

where k is the number of independent variables.

It reveals that the F-ratio is 1182 which is significant at p \< 0.0001. This result tells us that there are less than 0.01% chance that an F-ratio this large will happen if the null hypothesis is true. Therefore, we can conclude that our regression model result is significantly better explanatory model of *SELLING PRICE* than if we used the mean value of selling prices. In short, the regression model overall estimates *SELLING PRICE* significantly well.

[For the intercept and slope values under the Coefficients section]{.underline}

i\. intercept

H0: α = 0

H1: α ≠ 0

ii\. slope

H0: β = 0

H1: β ≠ 0

The report reveals that the p-values of both the estimates of the Intercept and AREA_SQM are smaller than 0.001. In view of this, the null hypothesis of the α and β are equal to 0 will be rejected. As a result, we infer that the α and β are good parameter estimates.

To visualise the best fit curve on a scatterplot, we can incorporate `lm()` as a method function in ggplot as shown in the code chunk below.

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm) +
  ggtitle("LM plot of Selling Price vs Area")
```

The chart above reveals that there are a few outliers with relatively high selling prices.

### 6.8.2 Multiple Linear Regression Method

#### 6.8.2.1 Visualising the relationships of the independent variables

Before building a multiple regression model, it is important to ensure that the independent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as **multicollinearity** in statistics.

Correlation matrix is commonly used to visualise the relationships between the independent variables. Beside the `pairs()` of R, many packages support the display of a correlation matrix. In this section, the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package is used.

The code chunk below is used to plot a matrix of the relationship between the independent variables in *condo_resale* data.frame.

```{r fig.height=10, fig.width=10}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

::: callout-tip
## We have to pass in a non-sf data frame in order to create the correlation matrix plot above
:::

Things to note:

-   The matrix *order* argument is very important for determining the hidden structure and pattern in the matrix. There are four methods in corrplot (parameter order), named "AOE", "FPC", "hclust", "alphabet". In the code chunk above, AOE order is used. It orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

-   From the plotted matrix, it is clear that *Freehold* is highly correlated to *LEASE_99YEAR*. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, *LEASE_99YEAR* is excluded in the subsequent model building.

### 6.8.3 Build a hedonic pricing model using multiple linear regression method

The code chunk below using `lm()` to calibrate the multiple linear regression model.

```{r}
# Fit the regrssion model
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)

# Display the results
summary(condo.mlr)
```

### 6.8.4 Prepare Publication Quality Table: olsrr method

With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revise the model by removing those variables which are not statistically significant.

::: callout-note
## Alternatively, we can conduct a stepwsie regression to identity the more significant independent variables.
:::

Now, we are ready to calibrate a revised model by using the code chunk below.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)

ols_regress(condo.mlr1)
```

Things to note from the report above:

-   Under Model Summary: We look at the Adjusted R-square and model explains 0.65 of the variation in selling price

-   Under ANOVA: We look at the Sig and the corresponding p-value (=0.000) which means the model is significant

-   Under Parameter Estimates: We look at the Beta values and their Sig (or p-value). To be mindful of the sign (+\|-) of the Beta estimates as it reflects the direction of the relationship between the independent and dependent variables.

### 6.8.5 Prepare Publication Quality Table: gtsummary method

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) package provides an elegant and flexible way to create publication-ready summary tables in R.

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

With the gtsummary package, model statistics can be included in the report by either appending them to the report table by using [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) or adding as a table source note by using [`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) as shown in the code chunk below.

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

The additional statistics have been added at the bottom of the report.

For more customisation options, refer to [Tutorial: tbl_regression](https://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)

#### 6.8.5.1 Checking for multicolinearity

In this section, we would use a fantastic R package specially programmed for performing OLS regression. It is called [**olsrr**](https://olsrr.rsquaredacademy.com/). It provides a collection of very useful methods for building better multiple linear regression models:

-   comprehensive regression output

-   residual diagnostics

-   measures of influence

-   heteroskedasticity tests

-   collinearity diagnostics

-   model fit assessment

-   variable contribution assessment

-   variable selection procedures

In the code chunk below, the [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) of **olsrr** package is used to test if there are sign of multicollinearity.

::: callout-note
## We conduct this 2nd round of multicollinearity after fitting the lm model using the VIF measure.
:::

```{r}
ols_vif_tol(condo.mlr1)
```

VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. R-squared value is determined to find out how well an independent variable is described by the other independent variables. A high value of R-squared means that the variable is highly correlated with the other variables.

-   VIF starts at 1 and has no upper limit

-   VIF = 1, no correlation between the independent variable and the other variables

-   VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others

Since the VIF of the independent variables is less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.

#### 6.8.5.2 Test for Non-Linearity

In multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.

In the code chunk below, the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) of **olsrr** package is used to perform linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

The chart above reveals that most of the data poitns are scattered around the 0 line. Hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.

#### 6.8.5.3 Test for Normality Assumption of the residual errors

Lastly, the code chunk below uses [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of *olsrr* package to perform normality assumption test.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) resembles normal distribution.

For formal statistical test methods, the [`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html) of **olsrr** package can be used as shown in the code chun below.

```{r}
#| warning: false
ols_test_normality(condo.mlr1)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residuals are not normally distributed.

#### 6.8.5.4 Testing for Spatial Autocorrelation

The hedonic model we try to build uses geographically referenced attributes, hence it is also important for us to visualise the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert *condo_resale.sf* from sf data frame into a **SpatialPointsDataFrame**.

First, we will export the residual of the hedonic pricing model and save it as a data frame.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

Next, we will join the newly created data frame with *condo_resale.sf* object and rename the newly joined column.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
                  rename(`MLR_RES` = `condo.mlr1.residuals`)
```

Next, we convert *condo_resale.res.sf* from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

The code chunk below will be used to perform the data conversion process.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

Next, we use **tmap** package to display the distribution of the residuals on an interactive map.

The code churn below will turn on the interactive mode of **tmap**.

```{r}
tmap_mode("view")
```

The code chunks below is used to create an interactive point symbol map.

```{r}
#| warning: false
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
  tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

We switch back to "plot" mode before we continue.

```{r}
tmap_mode("plot")
```

The figure above reveals that there is sign of spatial autocorrelation.

To prove that our observation is indeed true, the Moran's I test will be performed

First, we will compute the distance-based weight matrix by using [`dnearneigh()`](https://r-spatial.github.io/spdep/reference/dnearneigh.html) function of **spdep**.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

Next, [`nb2listw()`](https://r-spatial.github.io/spdep/reference/nb2listw.html) of **spdep** packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

We use [`lm.morantest()`](https://r-spatial.github.io/spdep/reference/lm.morantest.html) of **spdep** package to perform Moran's I test for residual spatial autocorrelation.

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we reject the null hypothesis that the residuals are randomly distributed.

Since the observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.

## 6.9 Building Hedonic Pricing Models using GWmodel

In this section, we learn how to model hedonic pricing using both the fixed and adaptive bandwidth schemes

### 6.9.1 Build Fixed Bandwidth GWR Model

#### 6.9.1.1 Compute fixed bandwith

In the code chunk below `bw.gwr()` of **GWModel** package is used to determine the optimal fixed bandwidth to use in the model.

Notice that the *adaptive* argument of the function is set to **FALSE** to indicate that we are interested to compute the fixed bandwidth.

There are two possible approaches to determine the stopping rule using the *approach* argument, they are:

-   CV cross-validation approach and

-   AIC corrected (AICc) approach.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

::: callout-tip
The *longlat* argument is set to "FALSE" because we have already transformed the projected coordinates earlier. Set to "TRUE" if we are using long, lat coordinates as inputs.
:::

The result shows that the recommended bandwidth is 971.3405 metres (refer to the last line of the report).

+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Quiz:**                                                                                                                                                       |
+=================================================================================================================================================================+
| *Do you know why it is in metres?*                                                                                                                              |
|                                                                                                                                                                 |
| Reply: The Projected CRS of SVY21 and transformed equivalent under EPSG:3414 for the URA Master Plan 2014's planning subzone boundaries are measured in metres. |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+

#### 6.9.1.2 GWModel method - fixed bandwith

Now we can use the code chunk below to calibrate the GWR model using fixed bandwidth and gaussian kernel.

```{r}
#| warning: false
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

The output is saved in a list of class "gwrm". The code below is used to display the model output.

```{r}
gwr.fixed
```

The report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.14.

### 6.9.2 Build Adaptive Bandwidth GWR Model

In this section, we calibrate the gwr-absed hedonic pricing model by using the adaptive bandwidth approach.

#### 6.9.2.1 Compute the adaptive bandwidth

Similar to the earlier section, we first use *bw.ger()* to determine the recommended data points to use.

The code chunk used look very similar to the one used to compute the fixed bandwidth except the *adaptive* argument has changed to TRUE.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

The result shows that the 30 is the recommended data points to be used.

#### 6.9.2.2 Constructing the adaptive bandwidth gwr model

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.

```{r}
#| warning: false
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

The code below can be used to display the model output.

```{r}
gwr.adaptive
```

The report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.

### 6.9.3 Decoding the GWR Outputs

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, explanatory variable coefficients and standard errors:

-   Condition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.

-   Local R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing in the regression model.

-   Predicted: these are the estimated (or fitted) y values computed by the GWR.

-   Residuals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of 0 and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produced by using these values.

-   Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its "data" slot in an object called SDF of the output list.

### 6.9.4 Convert SDF into *sf* data.frame

To visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

Next, we use `glimpse()` to display the content of *condo_resale.sf.adaptive* sf data frame.

```{r}
glimpse(condo_resale.sf.adaptive)
```

The predicted Selling Price for the transaction is summarised as follow:

```{r}
summary(gwr.adaptive$SDF$yhat)
```

### 6.9.5 Visualise Local R2

The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_mode("view")

tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
  tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```

We then turn off interactive view.

```{r}
tmap_mode("plot")
```

#### 6.9.5.1 Visualise the Local R2 by URA Planning Region

We can print the maps with Local R2 values by Planning Region

```{r}
#| warning: false
planning_region <- c("CENTRAL REGION","WEST REGION","EAST REGION","NORTH-EAST REGION", "NORTH REGION")

for (region in planning_region){
  print(tm_shape(mpsz_svy21[mpsz_svy21$REGION_N==region, ])+
    tm_polygons()+
    tm_shape(condo_resale.sf.adaptive) + 
    tm_bubbles(col = "Local_R2",
             size = 0.15,
             border.col = "gray60",
             border.lwd = 1) +
    tm_layout(main.title = paste("Local R2 for", region),
            main.title.position = "center",
            main.title.size = 1.0)
  )
  }
```

### 6.9.6 Visualise coefficient estimates

The code chunk below is used to create an interactive point symbol map.

```{r}
# Switch to interactive plot
tmap_mode("view")

# Plot the coefficient estimates
AREA_SQM_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

# We set sync = TRUE so that both maps move together when we select a location of the map.
tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=1, nrow = 2,
             sync = TRUE)
```

We then switch back to view mode

```{r}
tmap_mode("plot")
```

## 6.10 Reference

Gollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) "GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models". *Journal of Statistical Software*, 63(17):1-50, http://www.jstatsoft.org/v63/i17/

Lu B, Harris P, Charlton M, Brunsdon C (2014) "The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models". *Geo-spatial Information Science* 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453

Dr. Kam TIn Seong (2022) , "ISSS602 Data Analytics Lab Lesson 5: The Granddaddy of All Models: Regression Analysis" Version 2.14.0

\
